{
	"swagger": "2.0",
	"info": {
		"title": "Amazon S3",
		"description": "This is the Amazon Simple Storage Service API Reference. ",
		"termsOfService": null,
		"version": "2006-03-01"
	},
	"host": "s3.amazonaws.com",
	"basePath": "/",
	"schemes": [
		"http"
	],
	"produces": [
		"application/json"
	],
	"consumes": [
		"application/json"
	],
	"paths": {
		"/": {
			"put": {
				"summary": "PUT Bucket",
				"description": "This implementation of the PUT operation creates a new bucket. To create a bucket, you must register with Amazon S3 and have a valid AWS Access Key ID to authenticate requests. Anonymous requests are never allowed to create buckets. By creating the bucket, you become the bucket owner.Not every string is an acceptable bucket name. For information on bucket naming restrictions, see Working with Amazon S3 Buckets. By default, the bucket is created in the US Standard region. You can optionally specify a region in the request body. You might choose a region to optimize latency, minimize costs, or address regulatory requirements. For example, if you reside in Europe, you will probably find it advantageous to create buckets in the EU (Ireland) region. For more information, see How to Select a Region for Your Buckets. NoteIf you create a bucket in a region other than US Standard, your application must be able to handle 307 redirect. For more information, go to Virtual Hosting of Buckets in Amazon Simple Storage Service Developer Guide.When creating a bucket using this operation, you can optionally specify the accounts or groups that should be granted specific permissions on the bucket. There are two ways to grant the appropriate permissions using the request headers.Specify a canned ACL using the x-amz-acl request header. For more in formation, see Canned ACL in the Amazon Simple Storage Service Developer Guide. Specify access permissions explicitly using the x-amz-grant-read,x-amz-grant-write, x-amz-grant-read-acp,x-amz-grant-write-acp,x-amz-grant-full-control headers. These headers map to the set of permissions Amazon S3 supports in an ACL. For more information, go toAccess Control List (ACL) Overview in the AmazonSimple Storage Service Developer Guide.NoteYou can use either a canned ACL or specify access permissions explicitly. You cannot do both. ",
				"operationId": "put-bucket",
				"parameters": [
					{
						"name": "x-amz-acl",
						"in": "header",
						"description": "The canned ACL to apply to the bucket you are creating. For more information, go to Canned ACL in the Amazon Simple Storage Service Developer Guide.  Type: String Valid Values: private | public-read | public-read-write | authenticated-read | bucket-owner-read | bucket-owner-full-control",
						"type": "string"
					},
					{
						"name": "x-amz-grant-full-control",
						"in": "header",
						"description": "Allows grantee the READ, WRITE, READ_ACP, and WRITE_ACP permissions on the bucket. Type: String  Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-read",
						"in": "header",
						"description": "Allows grantee to list the objects in the bucket. Type: String Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-read-acp",
						"in": "header",
						"description": "Allows grantee to read the bucket ACL. Type: String Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-write",
						"in": "header",
						"description": "Allows grantee to create, overwrite, and delete any object in the bucket. Type: String Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-write-acp",
						"in": "header",
						"description": "Allows grantee to write the ACL for the applicable bucket. Type: String Default: None Constraints: None",
						"type": "string"
					}
				],
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?acl": {
			"put": {
				"summary": "PUT Bucket acl",
				"description": "This implementation of the PUT operation uses the acl subresource to set the permissions on an existing bucket using access control lists(ACL). For more information, go to Using ACLs. To set the ACL of a bucket, you must have WRITE_ACP permission. You can use one of the following two ways to set a bucket's permissions:Specify the ACL in the request bodySpecify permissions using request headers NoteYou cannot specify access permission using both the body and the request headers. Depending on your application needs, you may choose to set the ACL on a bucket using either the request body or the headers. For example, if you have an existing application that updates a bucket ACL using the request body, then you can continue to use that approach.  ",
				"operationId": "put-bucket-acl",
				"parameters": [
					{
						"name": "x-amz-acl",
						"in": "header",
						"description": "Sets the ACL of the bucket using the specified canned ACL. Type: String Valid Values: private | public-read | public-read-write | authenticated-read Default: private",
						"type": "string"
					},
					{
						"name": "x-amz-grant-full-control",
						"in": "header",
						"description": "Allows the specified grantee(s) the READ, WRITE, READ_ACP, and WRITE_ACP permissions on the bucket. Type: String Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-read",
						"in": "header",
						"description": "Allows the specified grantee(s) to list the objects in the bucket. Type: String Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-read-acp",
						"in": "header",
						"description": "Allows the specified grantee(s) to read the bucket ACL. Type: String  Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-write",
						"in": "header",
						"description": "Allows the specified grantee(s) to create, overwrite, and delete any object in the bucket. Type: String Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-write-acp",
						"in": "header",
						"description": "Allows the specified grantee(s) to write the ACL for the applicable bucket. Type: String t Default: None Constraints: None",
						"type": "string"
					}
				],
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?cors": {
			"get": {
				"summary": "GET Bucket cors",
				"description": "Returns the cors configuration information set for the bucket.To use this operation, you must have permission to perform the s3:GetBucketCORSaction. By default, the bucket owner has this permission and can grant it to others.To learn more cors, go to EnablingCross-Origin Resource Sharing in the Amazon Simple Storage Service Developer Guide.",
				"operationId": "get-bucket-cors",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?delete": {
			"post": {
				"summary": "Delete Multiple Objects",
				"description": "The Multi-Object Delete operation enables you to delete multiple objects from a bucket using a single HTTP request. If you know the object keys that you want to delete, then this operation provides a suitable alternative to sending individual delete requests(see DELETE Object), reducing per-request overhead. The Multi-Object Delete request contains a list of up to 1000 keys that you want to delete.In the XML, you provide the object key names, and optionally, version IDs if you want to delete a specific version of the object from a versioning-enabled bucket. For each key,Amazon S3 performs a delete operation and returns the result of that delete, success, or failure, in the response. Note that, if the object specified in the request is not found, Amazon S3 returns the result as deleted.The Multi-Object Delete operation supports two modes for the response; verbose and quiet.By default, the operation uses verbose mode in which the response includes the result of deletion of each key in your request. In quiet mode the response includes only keys where the delete operation encountered an error. For a successful deletion, the operation does not return any information about the delete in the response body. When performing a Multi-Object Delete operation on an MFA Delete enabled bucket, that attempts to delete any versioned objects, you must include an MFA token. If you do not provide one, the entire request will fail, even if there are non versioned objects you are attempting to delete. If you provide an invalid token, whether there are versioned keys in the request or not, the entire Multi-Object Delete request will fail. For information about MFA Delete, see MFA Delete.Finally, the Content-MD5 header is required for all Multi-Object Delete requests. Amazon S3 uses the header value to ensure that your request body has not be altered in transit. ",
				"operationId": "delete-multiple-objects",
				"parameters": [
					{
						"name": "Content-Length",
						"in": "header",
						"description": "Length of the body according to RFC 2616. Type: String Default: None",
						"type": "string"
					},
					{
						"name": "Content-MD5",
						"in": "header",
						"description": "The base64-encoded 128-bit MD5 digest of the data. This header must be used as a message integrity check to verify that the request body was no corrupted in transit. For more information, go to RFC t1864. Type: String Default: None",
						"type": "string"
					},
					{
						"name": "x-amz-mfa",
						"in": "header",
						"description": "The value is the concatenation of the authentication devices serial number, a space, and the value that is displayed on your authentication device. Type: String Default: None  tCondition: Required to permanently delete a versioned object if versioning is configured with MFA Delete enabled.",
						"type": "string"
					}
				],
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?lifecycle": {
			"put": {
				"summary": "PUT Bucket lifecycle",
				"description": "Creates a new lifecycle configuration for the bucket or replaces an existing lifecycle configuration.For information about lifecycle configuration, go to Object Lifecycle Management in the Amazon Simple Storage Service Developer Guide.",
				"operationId": "put-bucket-lifecycle",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?location": {
			"put": {
				"summary": "PUT Bucket logging",
				"description": "NoteThe logging implementation of PUT Bucket is a beta feature.This implementation of the PUT operation uses the logging subresource to set the logging parameters for a bucket and to specify permissions for who can view and modify the logging parameters. To set the logging status of a bucket, you must be the bucket owner.The bucket owner is automatically granted FULL_CONTROL to all logs. You use theGrantee request element to grant access to other people. ThePermissions request element specifies the kind of access the grantee has to the logs.To enable logging, you use LoggingEnabled and its children request elements.To disable logging, you use an empty BucketLoggingStatusrequest element:&lt;BucketLoggingStatus xmlns=http://doc.s3.amazonaws.com/2006-03-01 /&gt;For more information about creating a bucket, see PUTBucket. For more information about returning the logging status of a bucket,see GET Bucket logging.",
				"operationId": "put-bucket-logging",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?logging": {
			"put": {
				"summary": "PUT Bucket notification",
				"description": "The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. For more information about event notifications, go to Configuring Event Notifications in the Amazon Simple Storage Service Developer Guide. Using this API, you can replace an existing notification configuration. The configuration is an XML file that defines the event types that you want Amazon S3 to publish and the destination where you want Amazon S3 to publish an event notification when it detects an event of the specified type. By default, your bucket has no event notifications configured. That is, the notification configuration will be an emptyNotificationConfiguration.&lt;NotificationConfiguration&gt;&lt;/NotificationConfiguration&gt;This operation replaces the existing notification configuration with the configuration you include in the request body. After Amazon S3 receives this request, it first verifies that any SNS or SQS destination exists, and that the bucket owner has permission to publish to it by sending a test notification. In the case of Lambda destinations, Amazon S3 will verify that the actor submitting the configuration has permissions to pass the invocation role specified, andAmazon S3 can assume the role. For more information, go to Configuring Notifications for Amazon S3Events in the Amazon Simple Storage Service Developer Guide.You can disable notification by adding the emptyNotificationConfiguration element.  By default, only the bucket owner can configure notifications on a bucket. However,bucket owners can use a bucket policy to grant permission to other users to set this configuration with s3:PutBucketNotification permission.NoteThe PUT notification is an atomic operation. For example, suppose your notification configuration includes SNS topic, SQS queue, and Lambda function configurations. When you send a PUT request with this configuration, Amazon S3sends test messages to your SNS topic. If the message fails, the entire PUToperation will fail, and Amazon S3 will not add the configuration to your bucket.",
				"operationId": "put-bucket-notification",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?notification": {
			"get": {
				"summary": "GET Bucket notification",
				"description": "This implementation of the GET operation uses the notification subresource to return the notification configuration of a bucket. If notifications are not enabled on the bucket, the operation returns an emptyNotificationConfiguration element.By default, you must be the bucket owner to read the notification configuration of a bucket.However, the bucket owner can use a bucket policy to grant permission to other users to read this configuration with the s3:GetBucketNotificationpermission.For more information about setting and reading the notification configuration on a bucket,see Setting Up Notification of Bucket Events. For more information about bucket policies, see Using Bucket Policies.",
				"operationId": "get-bucket-notification",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?policy": {
			"put": {
				"summary": "PUT Bucket policy",
				"description": "This implementation of the PUT operation uses the policy subresource to add to or replace a policy on a bucket. If the bucket already has a policy, the one in this request completely replaces it. To perform this operation, you must be the bucket owner.If you are not the bucket owner but have PutBucketPolicy permissions on the bucket, Amazon S3 returns a 405 Method Not Allowed. In all other cases for a PUT bucket policy request that is not from the bucket owner, Amazon S3returns 403 Access Denied. There are restrictions about who can create bucket policies and which objects in a bucket they can apply to. For more information,go to Using Bucket Policies.",
				"operationId": "put-bucket-policy",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?requestPayment": {
			"put": {
				"summary": "PUT Bucket requestPayment",
				"description": "This implementation of the PUT operation uses therequestPayment subresource to set the request payment configuration of a bucket. By default, the bucket owner pays for downloads from the bucket. This configuration parameter enables the bucket owner (only) to specify that the person requesting the download will be charged for the download. For more information,see Requester Pays Buckets.",
				"operationId": "put-bucket-requestpayment",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?tagging": {
			"put": {
				"summary": "PUT Bucket tagging",
				"description": "This implementation of the PUT operation uses the tagging subresource to add a set of tags to an existing bucket.Use tags to organize your AWS bill to reflect your own cost structure.To do this, sign up to get your AWS account bill with tag key values included.Then, to see the cost of combined resources, organize your billing information according to resources with the same tag key values. For example, you can tag several resources with a specific application name, and then organize your billing information to see the total cost of that application across several services.For more information, see Cost Allocation and Tagging in About AWS Billing and Cost Management.To use this operation, you must have permission to perform thes3:PutBucketTagging action. By default, the bucket owner has this permission and can grant this permission to others. ",
				"operationId": "put-bucket-tagging",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?uploads": {
			"get": {
				"summary": "List Multipart Uploads",
				"description": "This operation lists in-progress multipart uploads. An in-progress multipart upload is a multipart upload that has been initiated, using the Initiate Multipart Upload request,but has not yet been completed or aborted. This operation returns at most 1,000 multipart uploads in the response. 1,000 multipart uploads is the maximum number of uploads a response can include, which is also the default value. You can further limit the number of uploads in a response by specifying the max-uploads parameter in the response. If additional multipart uploads satisfy the list criteria, the response will contain anIsTruncated element with the value true. To list the additional multipart uploads, use the key-marker and upload-id-marker request parameters.In the response, the uploads are sorted by key. If your application has initiated more than one multipart upload using the same object key, then uploads in the response are first sorted by key. Additionally,  uploads are sorted in ascending order within each key by the upload initiation time. For more information on multipart uploads, go to Uploading Objects Using Multipart Upload in the Amazon S3Developer Guide.For information on permissions required to use the multipart upload API, go to Multipart Upload API and Permissions in the Amazon Simple Storage Service Developer Guide .",
				"operationId": "list-multipart-uploads",
				"parameters": [
					{
						"name": "delimiter",
						"in": "query",
						"description": "Character you use to group keys.  All keys that contain the same string between the prefix, if specified, and the first occurrence of the delimiter after the prefix are grouped under a single result element, ttCommonPrefixes. If you dont specify the prefix parameter, then the substring starts at the beginning of the key. The keys that are grouped under CommonPrefixes result element are not returned elsewhere in the response. T",
						"type": "string"
					},
					{
						"name": "encoding-type",
						"in": "query",
						"description": "Requests Amazon S3 to encode the response and specifies the encoding method to use. An object key can contain any Unicode character; however, XML 1.0 parser cannot parse some characters, such as characters with an ASCII value from 0 to 10. For characters that are not supported in XML 1.0, you can add this parameter to request that Amazon S3 encode the keys in the response. Type: String Default: None Valid value: url",
						"type": "string"
					},
					{
						"name": "key-marker",
						"in": "query",
						"description": "Together with upload-id-marker, this parameter specifies the multipar upload after which listing should begin.  If upload-id-marker is not specified, only the keys lexicographically greater than the specified key-marker twill be included in the list.  If upload-id-marker is specified, any multipart uploads for a key equal to the key-marker might also be included, provided those multipart uploads have upload IDs lexicographically great",
						"type": "string"
					},
					{
						"name": "max-uploads",
						"in": "query",
						"description": "Sets the maximum number of multipart uploads, from 1 to 1,000, to return in the response body. 1,000 is the maximum number of uploads that can tbe returned in a response. Type: Integer Default: 1,000",
						"type": "string"
					},
					{
						"name": "prefix",
						"in": "query",
						"description": "Lists in-progress uploads only for those keys that begin with the specified prefix. tYou can use prefixes to separate a bucket into different grouping of keys. (You can think of using prefix to make groups tin the same way youd use a folder in a file system.)  Type: String",
						"type": "string"
					},
					{
						"name": "upload-id-marker",
						"in": "query",
						"description": "Together with key-marker, specifies the multipart upload after which listing should begin. If key-marker is no specified, the upload-id-marker parameter is ignored. Otherwise, any multipart uploads for a key equal to the key-marker might be included in the list only if they have an upload ID lexicographically greater than the specified upload-id-marker.  Type: String",
						"type": "string"
					}
				],
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?versioning": {
			"put": {
				"summary": "PUT Bucket versioning",
				"description": "This implementation of the PUT operation uses the versioning subresource to set the versioning state of ane xisting bucket. To set the versioning state, you must be the bucket owner.You can set the versioning state with one of the following values:Enabled&#8212;Enables versioning for the objects in the bucketAll objects added to the bucket receive a unique version ID.Suspended&#8212;Disables versioning for the objects in the bucketAll objects added to the bucket receive the version IDnull.If the versioning state has never been set on a bucket, it has no versioning state; aGETversioning request does not return a versioning state value.If the bucket owner enables MFA Delete in the bucket versioning configuration, the bucket owner must include the x-amz-mfa request header and theStatus and the MfaDelete request elements in a request to set the versioning state of the bucket.For more information about creating a bucket, see PUTBucket. For more information about returning the versioning state of a bucket, see GET Bucket VersioningStatus.",
				"operationId": "put-bucket-versioning",
				"parameters": [
					{
						"name": "x-amz-mfa",
						"in": "header",
						"description": "The value is the concatenation of the authentication devices serial number, a space, and the value displayed on your authentication device. Type: String  Default: None Condition: Required to configure the versioning state if versioning is configured with MFA Delete enabled.",
						"type": "string"
					}
				],
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?versions": {
			"get": {
				"summary": "GET Bucket Object versions",
				"description": "You can use the versions subresource to list metadata about all of the versions of objects in a bucket. You can also use request parameters as selection criteria to return metadata about a subset of all the object versions. For more information, see RequestParameters.To use this operation, you must have READ access to the bucket.",
				"operationId": "get-bucket-object-versions",
				"parameters": [
					{
						"name": "delimiter",
						"in": "query",
						"description": "A delimiter is a character that you specify to group keys. All keys that contain the same string between the prefix and the first occurrence of the delimiter are grouped under a single result element in ttCommonPrefixes. These groups are counted as one result against the max-keys limitation. These keys are not returned elsewhere in the response. Also, see prefix. Type: String Default: None",
						"type": "string"
					},
					{
						"name": "encoding-type",
						"in": "query",
						"description": "Requests Amazon S3 to encode the response and specifies the encoding method to use. An object key can contain any Unicode character; however, XML 1.0 parser cannot parse some characters, such as characters with an ASCII value from 0 to 10. For characters that are not supported in XML 1.0, you can add this parameter to request that Amazon S3 encode the keys in the response.   Type: String Default: None Valid value: url",
						"type": "string"
					},
					{
						"name": "key-marker",
						"in": "query",
						"description": "Specifies the key in the bucket that you want to start listing from. Also, see version-id-marker. Type: String Default: None",
						"type": "string"
					},
					{
						"name": "max-keys",
						"in": "query",
						"description": "Sets the maximum number of keys returned in the response body. tThe response might contain fewer keys, but will never contain more. If  additional keys satisfy the search criteria, but were not returned because max-keys was exceeded, the response contains &lt;isTruncated&gt;true&lt;/isTruncated&gt;. To return the additional keys, see key-marker and version-id-marker. Type: String Default: 1000",
						"type": "string"
					},
					{
						"name": "prefix",
						"in": "query",
						"description": "Use this parameter to select only those keys that begin with the specified prefix. You can use prefixes to separate a bucket into different groupings of keys. (You can think of using prefix to make groups in the same way youd use a folder in a file system.) You can use prefix with delimiter to troll up numerous objects into a single result under CommonPrefixes. Also, see delimiter. Type: String Default: None",
						"type": "string"
					},
					{
						"name": "version-id-marker",
						"in": "query",
						"description": "Specifies the object version you want to start listing from. tAlso, see key-marker. Type: String Default: None Valid Values: Valid version ID | Default Constraint: May not be an empty string",
						"type": "string"
					}
				],
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/?website": {
			"put": {
				"summary": "PUT Bucket website",
				"description": "Sets the configuration of the website that is specified in the website subresource. To configure a bucket as a website, you can add this subresource on the bucket with website configuration information such as the file name of the index document and any redirect rules. For more information, go toHosting Websites on Amazon S3 in theAmazon Simple Storage Service Developer Guide.This PUT operation requires the S3:PutBucketWebsite permission. By default, only the bucket owner can configure the website attached to a bucket; however, bucket owners can allow other users to set the website configuration by writing a bucket policy that grants them the S3:PutBucketWebsite permission. ",
				"operationId": "put-bucket-website",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/destinationObject": {
			"put": {
				"summary": "PUT Object - Copy",
				"description": "This implementation of the PUT operation creates a copy of an object that is already stored in Amazon S3. A PUT copy operation is the same as performing aGET and then a PUT. Adding the request header,x-amz-copy-source, makes the PUT operation copy the source object into the destination bucket.NoteYou can store individual objects of up to 5 TB in Amazon S3. You create a copy of your object up to 5 GB in size in a single atomic operation using this API. However, for copying an object greater than 5 GB, you must use the multipart upload API. For conceptual information on multipart upload, go to Uploading Objects Using MultipartUpload in the Amazon Simple Storage Service Developer Guide.When copying an object, you can preserve most of the metadata (default) or specify new metadata. However, the ACL is not preserved and is set to private for the user making the request. All copy requests must be authenticated and cannot contain a message body.Additionally, you must have READ access to the source object and WRITE access to the destination bucket. For more information, see REST Authentication.To copy an object only under certain conditions, such as whether the ETagmatches or whether the object was modified before or after a specified date, use the request headers x-amz-copy-source-if-match,x-amz-copy-source-if-none-match,x-amz-copy-source-if-unmodified-since, orx-amz-copy-source-if-modified-since. NoteAll headers prefixed with x-amz- must be signed, includingx-amz-copy-source.You can use this operation to change the storage class of an object that is already stored in Amazon S3 using the x-amz-storage-class request header. For more information,go to Changing the Storage Class of anObject in Amazon S3 in the Amazon Simple Storage Service Developer Guide.The source object that you are copying can be encrypted or unencrypted. If the source object is encrypted, it can be encrypted by server-side encryption using AWS-managed encryption keys or by using a customer-provided encryption key. When copying an object,you can request that Amazon S3 encrypt the target object by using either the AWS-managed encryption keys or by using your own encryption key, regardless of what form of server-side encryption was used to encrypt the source or if the source object was not encrypted. For more information about server-side encryption, go to Using Server-SideEncryption in the Amazon Simple Storage Service Developer Guide. There are two opportunities for a copy request to return an error. One can occur whenAmazon S3 receives the copy request and the other can occur while Amazon S3 is copying the files.If the error occurs before the copy operation starts, you receive a standard Amazon S3 error. If the error occurs during the copy operation, the error response is embedded in the 200 OK response. This means that a200 OK response can contain either a success or an error. Make sure to design your application to parse the contents of the response and handle it appropriately. If the copy is successful, you receive a response that contains the information about the copied object.Note If the request is an HTTP 1.1 request, the response is chunk encoded.Otherwise, it will not contain the content-length and you will need to read the entire body. ",
				"operationId": "put-object--copy",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/ObjectName": {
			"put": {
				"summary": "PUT Object",
				"description": "This implementation of the PUT operation adds an object to a bucket. You must have WRITE permissions on a bucket to add an object to it.Amazon S3 never adds partial objects; if you receive a success response, Amazon S3 added the entire object to the bucket.Amazon S3 is a distributed system. If it receives multiple write requests for the same object simultaneously, it overwrites all but the last object written. Amazon S3 does not provide object locking; if you need this, make sure to build it into your application layer or use versioning instead.To ensure that data is not corrupted traversing the network, use theContent-MD5 header. When you use this header, Amazon S3 checks the object against the provided MD5 value and, if they do not match, returns an error.Additionally, you can calculate the MD5 while putting an object to Amazon S3 and compare there turned ETag to the calculated MD5 value. NoteTo configure your application to send the Request Headers prior to sending the request body, use the 100-continue HTTP status code. ForPUT operations, this helps you avoid sending the message body if the message is rejected based on the headers (e.g., because of authentication failure or redirect). For more information on the100-continue HTTP status code, go to Section 8.2.3 of http://www.ietf.org/rfc/rfc2616.txt.You can optionally request server-side encryption where Amazon S3 encrypts your data as it writes it to disks in its data centers and decrypts it for you when you access it.You have option to provide your own encryption key or use AWS-managed encryption keys.For more information, go to Using Server-Side Encryption in theAmazon Simple Storage Service Developer Guide.",
				"operationId": "put-object",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/ObjectName?acl": {
			"put": {
				"summary": "PUT Object acl",
				"description": "This implementation of the PUT operation uses the acl subresource to set the access control list (ACL) permissions for an object that already exists in a bucket. You must have WRITE_ACP permission to set the ACL of an object. You can use one of the following two ways to set an object's permissions:Specify the ACL in the request body, orSpecify permissions using request headersDepending on your application needs, you may choose to set the ACL on an object using either the request body or the headers. For example, if you have an existing application that updates an object ACL using the request body, then you can continue to use that approach. ",
				"operationId": "put-object-acl",
				"parameters": [
					{
						"name": "x-amz-acl",
						"in": "header",
						"description": "The canned ACL to apply to the object. For more information, go to Canned ACL in the Amazon Simple t.  Type: String  Valid Values: private | public-read | public-read-write | authenticated-read | bucket-owner-read | bucket-owner-full-control  Default: private",
						"type": "string"
					},
					{
						"name": "x-amz-grant-full-control",
						"in": "header",
						"description": "Allows the specified grantee the READ, WRITE, READ_ACP, and tWRITE_ACP permissions on the bucket. Type: String  Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-read",
						"in": "header",
						"description": "Allows the specified grantee to list the objects in the bucket. Type: String Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-read-acp",
						"in": "header",
						"description": "Allows the specified grantee to read the bucket ACL. Type: String  Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-write",
						"in": "header",
						"description": "Not applicable when granting access permissions on objects. You can use this when granting access permissions on buckets. Type: String Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-write-acp",
						"in": "header",
						"description": "Allows the specified grantee to write the ACL for the applicable bucket. Type: String  Default: None Constraints: None",
						"type": "string"
					}
				],
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/ObjectName?partNumber={PartNumber}&uploadId={UploadId}": {
			"put": {
				"summary": "Upload Part",
				"description": "This operation uploads a part in a multipart upload.  Note In this operation, you provide part data in your request. However, you have an option to specify your existing Amazon S3 object as a data source for the part you are uploading. To upload a part from an existing object, you use the Upload Part (Copy)operation. For more information, see Upload Part - Copy. You must initiate a multipart upload (see Initiate Multipart Upload) before you can upload any part. In response to your initiate request, Amazon S3 returns an upload ID, a unique identifier, that you must include in your upload part request.Part numbers can be any number from 1 to 10,000, inclusive. A part number uniquely identifies a part and also defines its position within the object being created. If you upload a new part using the same part number that was used with a previous part, the previously uploaded part is overwritten. Each part must be at least 5 MB in size, except the last part. There is no size limit on the last part of your multipart upload.To ensure that data is not corrupted when traversing the network, specify theContent-MD5 header in the upload part request. Amazon S3 checks the part data against the provided MD5 value. If they do not match, Amazon S3 returns an error. NoteAfter you initiate multipart upload and upload one or more parts, you must either complete or abort multipart upload in order to stop getting charged for storage of the uploaded parts. Only after you either complete or abort the multi part upload, Amazon S3 frees up the parts storage and stops charging you for it. For more information on multipart uploads, go to Multipart Upload Overview in the Amazon Simple Storage Service Developer Guide .For information on the permissions required to use the multipart upload API, go to Multipart Upload API andPermissions in the Amazon Simple Storage Service Developer Guide.You can optionally request server-side encryption where Amazon S3 encrypts your data as it writes it to disks in its data centers and decrypts it for you when you access it. You have the option of providing your own encryption key, or you can use the AWS-managed encryption keys.If you choose to provide your own encryption key, the request headers you provide in the request must match the headers you used in the request to initiate the upload by using Initiate Multipart Upload. For more information, go to Using Server-Side Encryption in the Amazon Simple Storage Service Developer Guide.",
				"operationId": "upload-part",
				"parameters": [
					{
						"name": "Content-Length",
						"in": "header",
						"description": "The size of the part, in bytes. For more information, go to http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.13. Type: Integer Default: None",
						"type": "string"
					},
					{
						"name": "Content-MD5",
						"in": "header",
						"description": "The base64-encoded 128-bit MD5 digest of the part data. This header can be used as ta message integrity check to verify that the part data is the same data that was originally sent. Although it is optional, we recommend using the Content-MD5 mechanism as an end-to-end integrity check. For more information, see RFC t1864. Type: String Default: None",
						"type": "string"
					},
					{
						"name": "Expect",
						"in": "header",
						"description": "When your application uses 100-continue, it does not send the request body until it receives an acknowledgement. If the message is rejected based on the headers, the body of the message is not sent. For more information, go to RFC t2616. Type: String Default: None Valid Values: 100-continue",
						"type": "string"
					},
					{
						"name": "x-amz-copy-source",
						"in": "header",
						"description": "The name of the source bucket and the source object key name separated by a slash t(/). Type: String Default: None",
						"type": "string"
					},
					{
						"name": "x-amz-copy-source-server-side-encryption-customer-algorithm",
						"in": "header",
						"description": "Specifies algorithm to use when decrypting the source object. Type: String Default: None Valid Value: AES256 Constraints: Must be accompanied by a valid x-amz-copy-source-server-side-encryption-customer-key and x-amz-copy-source-server-side-encryption-customer-key-MD5 headers.",
						"type": "string"
					},
					{
						"name": "x-amz-copy-source-server-side-encryption-customer-key",
						"in": "header",
						"description": "Specifies the customer provided base-64 encoded encryption key for Amazon S3 to use to decrypt the source object. The encryption key provided in this header must be one that was used when the source object was created. Type: String Default: None Constraints: Must be accompanied by a valid x-amz-copy-source-server-side-encryption-customer-algorithm and x-amz-copy-source-server-side-encryption-customer-key",
						"type": "string"
					},
					{
						"name": "x-amz-copy-source-server-side-encryption-customer-key-MD5",
						"in": "header",
						"description": "Specifies the base64-encoded 128-bit MD5 digest of the encryption key according to RFC 1321. Amazon S3 uses this header for a message integrity check to ensure the encryption key was transmitted without error. Type: String Default: None Constraints: Must be accompanied by a valid x-amz-copy-source-server-side-encryption-customer-algorithm and x-amz-copy-source-server-side-encryption-customer-ke",
						"type": "string"
					},
					{
						"name": "x-amz-copy-source-if-match",
						"in": "header",
						"description": "Perform a copy if the source object entity tag (ETag) matches the specified value. tIf the value does not match, Amazon S3 returns an HTTP status code 412 precondition failed error. Type: String Default: None",
						"type": "string"
					},
					{
						"name": "x-amz-copy-source-if-modified-since",
						"in": "header",
						"description": "Perform a copy if the source object is modified after the time specified using this header. If the source object is not modified, Amazon S3 returns tan HTTP status code 412 precondition failed terror.  Type: String Default: None",
						"type": "string"
					},
					{
						"name": "x-amz-copy-source-if-none-match",
						"in": "header",
						"description": "Perform a copy if the source object entity tag (ETag) is different than the value specified using this header. If the values match, Amazon S3 returns an HTTP status code 412 precondition failed  error.  Type: String Default: None",
						"type": "string"
					},
					{
						"name": "x-amz-copy-source-if-unmodified-since",
						"in": "header",
						"description": "Perform a copy if the source object is not modified after the time specified using this header. If the source object is modified, Amazon S3 returns tan HTTP status code 412 precondition failed terror.  Type: String Default: None",
						"type": "string"
					},
					{
						"name": "x-amz-copy-source-range",
						"in": "header",
						"description": "The range of bytes to copy from the source object. The range value must use the form bytes=first-last, where the first and last are the zero-based byte offsets to copy. For example, bytes=0-9 indicates that you want to copy the first ten bytes of the source. This request header is not required when copying an entire source object.  Type: Integer Default: None",
						"type": "string"
					},
					{
						"name": "x-amz-server-side-encryption",
						"in": "header",
						"description": "Specifies a server-side encryption algorithm to use when Amazon S3 creates an object.  Type: String Valid Value: AES256",
						"type": "string"
					},
					{
						"name": "x-amz-server-side-encryption-customer-algorithm",
						"in": "header",
						"description": "Specifies the algorithm to use to when encrypting the object. Type: String Default: None Valid Value: AES256 Constraints: Must be accompanied by valid x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers.",
						"type": "string"
					},
					{
						"name": "x-amz-server-side-encryption-customer-key",
						"in": "header",
						"description": "Specifies the customer-provided base64-encoded encryption key for Amazon S3 to use in encrypting data. This value is used to store the object and then is discarded; Amazon does no store the encryption key. The key must be appropriate for use with the algorithm specified in the x-amz-server-side-encryption-customer-algorithm header. Type: String Default: None Cons",
						"type": "string"
					},
					{
						"name": "x-amz-server-side-encryption-customer-key-MD5",
						"in": "header",
						"description": "Specifies the base64-encoded 128-bit MD5 digest of the encryption key according to RFC 1321. Amazon S3 uses this header for a message integrity check to ensure the encryption key was transmitted without error. Type: String Default: None Constraints: Must be accompanied by valid x-amz-server-side-encryption-customer-algorithm and  x-amz-server-side-encryption-customer-key headers.",
						"type": "string"
					}
				],
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/ObjectName?restore&amp;versionId=VersionID": {
			"post": {
				"summary": "POST Object restore",
				"description": "Restores a temporary copy of an archived object. You can optionally provide version ID to restore specific object version. If version ID is not provided, it will restore the current version.In the request, you specify the number of days that you want the restored copy to exist. After the specified period, Amazon S3 deletes the temporary copy. Note that the object remains archived; Amazon S3 deletes only the restored copy. An object in the Glacier storage class is an archived object. To access the object, you must first initiate a restore request, which restores a copy of the archived object.  Restore jobs typically complete in three to five hours. For more information about archiving objects, go to Object Lifecycle Management in Amazon Simple Storage Service Developer Guide.You can obtain restoration status by sending a HEAD request. In the response, these operations return the x-amz-restore header with restoration status information.After restoring an object copy, you can update the restoration period by reissuing this request with the new period. Amazon S3 updates the restoration period relative to the current time and charges only for the request, and there are no data transfer charges.You cannot issue another restore request when Amazon S3 is actively processing your first restore request for the same object; however, after Amazon S3 restores a copy of the object, you can send restore requests to update the expiration period of the restored object copy.If your bucket has a lifecycle configuration with a rule that includes an expiration action, the object expiration overrides the life span that you specify in a restore request. For example, if you restore an object copy for 10 days but the object is scheduled to expire in 3 days, Amazon S3 deletes the object in 3 days. For more information about lifecycle configuration, see PUT Bucket lifecycle.To use this action, you must have s3:RestoreObject permissions on the specified object. For more information, go to Access Control section in the Amazon S3 DeveloperGuide.",
				"operationId": "post-object-restore",
				"parameters": [
					{
						"name": "Content-MD5",
						"in": "header",
						"description": "The base64-encoded 128-bit MD5 digest of the data. This header must be used as a message integrity check to verify that the request body was not corrupted in transit. For more information, go to RFC tt1864. Type: String  Default: None",
						"type": "string"
					}
				],
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/ObjectName?torrent": {
			"get": {
				"summary": "GET Object torrent",
				"description": "This implementation of the GET operation uses the torrent subresource to return torrent files from a bucket.BitTorrent can save you bandwidth when you're distributing large files. For more information about BitTorrent, see Amazon S3 Torrent.NoteYou can get torrent only for objects that are less than 5 GB in size and that are not encrypted using server-side encryption with customer-provided encryption key.To use GET, you must have READ access to the object.",
				"operationId": "get-object-torrent",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/ObjectName?uploadId=UploadId": {
			"post": {
				"summary": "Complete Multipart Upload",
				"description": "This operation completes a multipart upload by assembling previously uploaded parts. You first initiate the multipart upload and then upload all parts using the Upload Parts operation (see Upload Part).After successfully uploading all relevant parts of an upload, you call this operation to complete the upload. Upon receiving this request, Amazon S3 concatenates all the parts in ascending order by part number to create a new object. In the Complete MultipartUpload request, you must provide the parts list. You must ensure the parts list is complete, this operation concatenates the parts you provide in the list. For each part in the list, you must provide the part number and the ETag header value, returned after that part was uploaded. Processing of a Complete Multipart Upload request could take several minutes to complete.After Amazon S3 begins processing the request, it sends an HTTP response header that specifies a 200 OK response. While processing is in progress, Amazon S3periodically sends whitespace characters to keep the connection from timing out. Because a request could fail after the initial 200 OK response has been sent, it is important that you check the response body to determine whether the request succeeded.Note that if Complete Multipart Upload fails, applications should be prepared to retry the failed requests. For more information, go to Amazon S3 Error Best Practices section of the Amazon Simple Storage Service Developer Guide .  For more information on multipart uploads, go to Uploading Objects Using Multipart Upload in the Amazon Simple Storage Service Developer Guide .For information on permissions required to use the multipart upload API, go to Multipart Upload API and Permissions in the Amazon Simple Storage Service Developer Guide .",
				"operationId": "complete-multipart-upload",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"/ObjectName?uploads": {
			"post": {
				"summary": "Initiate Multipart Upload",
				"description": "This operation initiates a multipart upload and returns an upload ID. This upload ID is used to associate all the parts in the specific multipart upload. You specify this upload IDin each of your subsequent upload part requests (see Upload Part). You also include this upload ID in the final request to either complete or abort the multipart upload request.For more information on multipart uploads, go to Multipart Upload Overview in the Amazon Simple Storage Service Developer Guide. For information on permissions required to use the multipart upload API, go to Multipart Upload API and Permissions in the Amazon Simple Storage Service Developer Guide.For request signing, multipart upload is just a series of regular requests, you initiate multipart upload, send one or more requests to upload parts, and finally complete multipart upload. You sign each request individually, there is nothing special about signing multipart upload requests. For more information about signing, see Authenticating Requests (AWS Signature Version 4). Note After you initiate multipart upload and upload one or more parts, you must either complete or abort multipart upload in order to stop getting charged for storage of the uploaded parts. Only after you either complete or abort multipart upload, Amazon S3 frees up the parts storage and stops charging you for the parts storage.You can optionally request server-side encryption where Amazon S3 encrypts your data as it writes it to disks in its data centers and decrypts it for you when you access it. You have the options of providing your own encryption key, using AWS Key Management Service (KMS) encryption keys, or the Amazon S3-managed encryption keys. If you choose to provide your own encryption key, the request headers you provide in the request must match the headers you used in the request to initiate the upload by using Initiate Multipart Upload. For more information, go to Protecting Data Using Server-Side Encryption in the Amazon Simple Storage Service Developer Guide.",
				"operationId": "initiate-multipart-upload",
				"parameters": [
					{
						"name": "Cache-Control",
						"in": "header",
						"description": "Can be used to specify caching behavior along the request/reply chain. For more information, go to http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9. Type: String Default: None",
						"type": "string"
					},
					{
						"name": "Content-Disposition",
						"in": "header",
						"description": "Specifies presentational information for the object. For more information, go to http://www.w3.org/Protocols/rfc2616/rfc2616-sec19.html#sec19.5.1. Type: String Default: None",
						"type": "string"
					},
					{
						"name": "Content-Encoding",
						"in": "header",
						"description": "Specifies what content encodings have been applied to the object and thus what decoding mechanisms must be applied to obtain the media-type referenced by the Content-Type header field. For more information, go to http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.11. Type: String Default: None",
						"type": "string"
					},
					{
						"name": "Content-Type",
						"in": "header",
						"description": "A standard MIME type describing the format of the object data. For more information, go to http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.17.   Type: String Default: binary/octel-stream  Constraints: MIME types only",
						"type": "string"
					},
					{
						"name": "Expires",
						"in": "header",
						"description": "The date and time at which the object is no longer cacheable. For more information, go to http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.21. Type: String Default: None",
						"type": "string"
					},
					{
						"name": "x-amz-acl",
						"in": "header",
						"description": "The canned ACL to apply to the object. Type: String Default: private Valid Values: private | public-read | public-read-write | authenticated-read | bucket-owner-read | bucket-owner-full-control  Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-full-control",
						"in": "header",
						"description": "Allows grantee the READ, READ_ACP, and WRITE_ACP permissions on the object. Type: String Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-read",
						"in": "header",
						"description": "Allows grantee to read the object data and its metadata. Type: String Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-read-acp",
						"in": "header",
						"description": "Allows grantee to read the object ACL. Type: String Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-write",
						"in": "header",
						"description": "Not applicable. Type: String Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-grant-write-acp",
						"in": "header",
						"description": "Allows grantee to write the ACL for the applicable object. Type: String  Default: None Constraints: None",
						"type": "string"
					},
					{
						"name": "x-amz-meta-",
						"in": "header",
						"description": "Any header starting with this prefix is considered user metadata. It will be stored with the object and returned when you retrieve the object. Type: String Default: None",
						"type": "string"
					},
					{
						"name": "x-amz-server-side-encryption",
						"in": "header",
						"description": "Specifies a server-side encryption algorithm to use when Amazon S3 creates an object. Type: String Valid Value: aws:kms, AES256",
						"type": "string"
					},
					{
						"name": "x-amz-server-side-encryption-customer-algorithm",
						"in": "header",
						"description": "Specifies the algorithm to use to when encrypting the object. Type: String Default: None Valid Value: AES256 Constraints: Must be accompanied by valid x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers.",
						"type": "string"
					},
					{
						"name": "x-amz-server-side-encryption-customer-key",
						"in": "header",
						"description": "Specifies the customer-provided base64-encoded encryption key for Amazon S3 to use in encrypting data. This value is used to store the object and then is discarded; Amazon does no store the encryption key. The key must be appropriate for use with the algorithm specified in the x-amz-server-side-encryption-customer-algorithm header. Type: String Default: None Constraints",
						"type": "string"
					},
					{
						"name": "x-amz-server-side-encryption-customer-key-MD5",
						"in": "header",
						"description": "Specifies the base64-encoded 128-bit MD5 digest of the encryption key according to RFC 1321. Amazon S3 uses this header for message integrity check to ensure the encryption key was transmitted without error. Type: String Default: None Constraints: Must be accompanied by valid x-amz-server-side-encryption-customer-algorithm and  x-amz-server-side-encryption-customer-key headers.",
						"type": "string"
					},
					{
						"name": "x-amz-server-side-encryption-aws-kms-key-id",
						"in": "header",
						"description": "If the x-amz-server-side-encryption is present and has the value of aws:kms, this header specifices the ID of the AWS Key Management Service (KMS) master encryption key that was used for the object. Type: String",
						"type": "string"
					},
					{
						"name": "x-amz-storage-class",
						"in": "header",
						"description": "The type of storage to use for the object that is created after successful multipart upload. Type: String Valid Values: STANDARD | REDUCED_REDUNDANCY Default: STANDARD Constraints: You cannot specify GLACIER as the storage class. To transition objects to the GLACIER storage class you can use lifecycle configuration.",
						"type": "string"
					},
					{
						"name": "x-amz-website-redirect-location",
						"in": "header",
						"description": "If the bucket is configured as a website, redirects requests for this object to another object in the same bucket or to an external URL. Amazon tS3 stores the value of this header in the object metadata. For information about object metadata, go to Object Key and Metadata. In the following example, the request header sets the redirect to an object (anotherPage.html) in the same bucket: x-amz-website-redirect-location: t/anotherPage.html ",
						"type": "string"
					}
				],
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"?requestPayment": {
			"get": {
				"summary": "GET Bucket requestPayment",
				"description": "This implementation of the GET operation uses therequestPayment subresource to return the request payment configuration of a bucket. To use this version of the operation, you must be the bucket owner. For more information, see Requester Pays Buckets.",
				"operationId": "get-bucket-requestpayment",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		},
		"?versioning": {
			"get": {
				"summary": "GET Bucket versioning",
				"description": "This implementation of the GET operation uses the versioning subresource to return the versioning state of a bucket. To retrieve the versioning state of a bucket, you must be the bucket owner.This implementation also returns the MFA Delete status of the versioning state, i.e.,if the MFA Delete status is enabled, the bucket owner must use an authentication device to change the versioning state of the bucket.There are three versioning states:If you enabled versioning on a bucket, the response is:&lt;VersioningConfiguration xmlns=http://s3.amazonaws.com/doc/2006-03-01/&gt;  &lt;Status&gt;Enabled&lt;/Status&gt;&lt;/VersioningConfiguration&gt;If you suspended versioning on a bucket, the response is:&lt;VersioningConfiguration xmlns=http://s3.amazonaws.com/doc/2006-03-01/&gt;  &lt;Status&gt;Suspended&lt;/Status&gt;&lt;/VersioningConfiguration&gt;If you never enabled (or suspended) versioning on a bucket, the response is:&lt;VersioningConfiguration xmlns=http://s3.amazonaws.com/doc/2006-03-01//&gt;",
				"operationId": "get-bucket-versioning",
				"responses": {
					"200": {
						"description": "OK"
					}
				},
				"tags": [
					""
				]
			}
		}
	}
}
